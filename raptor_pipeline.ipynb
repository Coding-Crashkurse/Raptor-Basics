{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAPTOR-Style Hierarchical Summarization Demo\n",
    "\n",
    "This notebook walks through a lightweight RAPTOR workflow over the long-form text in `themen_text.txt`. We will:\n",
    "\n",
    "1. Split the document into manageable chunks.\n",
    "2. Embed the chunks with OpenAI embeddings (no vector store needed).\n",
    "3. Run PCA to project embeddings to 2D and visualize them.\n",
    "4. Cluster the chunks, visualize the clusters, and summarize each cluster with an LLM.\n",
    "5. Embed the summaries, cluster them again, and create a \"summary of summaries\" (the RAPTOR root).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4ad2c9",
   "metadata": {},
   "source": [
    "> **adRAP vs. klassisches RAPTOR**  \n",
    "> Der aktuelle Stand der Forschung (arXiv:2410.01736) beschreibt, wie **adRAP** die aus RAPTOR bekannte rekursive Baumstruktur f?r dynamische Datens?tze erh?lt. Statt den kompletten Baum nach jedem Dokument neu zu bauen, werden neue Chunks per adaptivem UMAP/GMM nur in die betroffenen Cluster einsortiert und ausschlie?lich deren Zusammenfassungen aktualisiert. Genau diese Idee greifen wir hier auf: Das Notebook demonstriert, wie wir Chroma als Vektorspeicher nutzen, um neue Texte incremental zu persistieren, nur die relevanten Cluster neu zu abstrahieren und damit Abfragen wie _?Zu welchen Themen hast du Informationen??_ jederzeit beantworten zu k?nnen ? ohne einen vollst?ndigen Rebuild starten zu m?ssen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install dependencies (one-time)\n",
    "\n",
    "Install the required packages with the following command if they are not already available in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    langchain-openai \\\n",
    "    langchain-text-splitters \\\n",
    "    langchain-core \\\n",
    "    openai \\\n",
    "    matplotlib \\\n",
    "    seaborn \\\n",
    "    scikit-learn \\\n",
    "    pandas \\\n",
    "    numpy \\\n",
    "    python-dotenv \\\n",
    "    langchain-chroma>=0.1.2 \\\n",
    "    chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment & imports\n",
    "\n",
    "Store your API keys securely via environment variables. The prompts below only run if a key is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the source document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_path = Path(\"themen_text.txt\")\n",
    "document = document_path.read_text(encoding=\"utf-8\")\n",
    "print(f\"Loaded {document_path} with {len(document.split())} words\")\n",
    "document[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split into chunks\n",
    "\n",
    "We use the requested `RecursiveCharacterTextSplitter` with `chunk_size=100` and `chunk_overlap=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "chunks = text_splitter.split_text(document)\n",
    "chunk_df = pd.DataFrame({\"chunk_id\": range(len(chunks)), \"text\": chunks})\n",
    "chunk_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embed chunks (no vector store)\n",
    "\n",
    "We use `text-embedding-3-large` via `OpenAIEmbeddings`. This cell may take a while depending on the number of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "chunk_vectors = np.array(embeddings_model.embed_documents(chunk_df[\"text\"].tolist()))\n",
    "chunk_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dimensionality reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=42)\n",
    "chunk_coords = pca.fit_transform(chunk_vectors)\n",
    "chunk_df[[\"pc1\", \"pc2\"]] = chunk_coords\n",
    "chunk_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(chunk_df[\"pc1\"], chunk_df[\"pc2\"], s=30, alpha=0.7)\n",
    "plt.title(\"Chunk embeddings projected with PCA\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhouette-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def best_k_by_silhouette(X, k_min=2, k_max=15, random_state=42):\n",
    "    scores = {}\n",
    "    for k in range(k_min, k_max + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=\"auto\")\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        score = silhouette_score(X, labels)\n",
    "        scores[k] = score\n",
    "        print(f\"k={k}: silhouette={score:.4f}\")\n",
    "    # bestes k zurÃ¼ckgeben\n",
    "    best_k = max(scores, key=scores.get)\n",
    "    return best_k, scores\n",
    "\n",
    "best_k, scores = best_k_by_silhouette(chunk_vectors, k_min=2, k_max=10)\n",
    "print(\"Bestes k nach Silhouette:\", best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clustering the chunks\n",
    "\n",
    "We keep it simple with K-Means. Adjust `n_clusters` to taste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 3\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=\"auto\")\n",
    "chunk_df[\"cluster\"] = kmeans.fit_predict(chunk_vectors)\n",
    "chunk_df.groupby(\"cluster\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(\n",
    "    data=chunk_df,\n",
    "    x=\"pc1\",\n",
    "    y=\"pc2\",\n",
    "    hue=\"cluster\",\n",
    "    palette=\"tab10\",\n",
    "    s=50,\n",
    ")\n",
    "plt.title(\"Chunk clusters after PCA\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summarize each cluster with an LLM\n",
    "\n",
    "The RAPTOR approach compresses each cluster into a higher-level node. We feed the concatenated chunk texts to a chat model and request a concise summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def _parse_cluster_summary(raw_text: str, cluster_id: int) -> tuple[str, str]:\n",
    "    \"\"\"Return (topic, summary_text).\"\"\"\n",
    "    text = raw_text.strip()\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "        topic = data.get(\"topic\") or data.get(\"title\") or f\"Cluster {cluster_id}\"\n",
    "        summary_text = data.get(\"summary\") or data.get(\"content\") or text\n",
    "        return topic.strip(), summary_text.strip()\n",
    "    except json.JSONDecodeError:\n",
    "        # fallback: first sentence as topic\n",
    "        parts = text.split(\"\n",
    "\", 1)\n",
    "        topic = parts[0][:80] if parts else f\"Cluster {cluster_id}\"\n",
    "        summary_text = text if len(parts) == 1 else parts[1]\n",
    "        return topic.strip() or f\"Cluster {cluster_id}\", summary_text.strip()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Summarize clusters and respond as compact JSON with keys 'topic' and 'summary'.\"),\n",
    "    (\n",
    "        \"user\",\n",
    "        \"You will receive the merged text for one cluster. Respond as JSON with keys 'topic' (<=12 words) and 'summary' (<=150 words) that captures the essential ideas.\n",
    "\n",
    "{cluster_text}\",\n",
    "    ),\n",
    "])\n",
    "\n",
    "cluster_summaries = []\n",
    "\n",
    "for cluster_id, group in chunk_df.groupby(\"cluster\"):\n",
    "    cluster_text = \"\n",
    "\n",
    "\".join(group[\"text\"].tolist())\n",
    "    response = llm.invoke(prompt.format_messages(cluster_text=cluster_text))\n",
    "    topic, summary_text = _parse_cluster_summary(response.content, int(cluster_id))\n",
    "    cluster_summaries.append(\n",
    "        {\n",
    "            \"cluster\": int(cluster_id),\n",
    "            \"topic\": topic,\n",
    "            \"summary\": summary_text,\n",
    "            \"raw\": response.content.strip(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "summary_df = pd.DataFrame(cluster_summaries).sort_values(\"cluster\").reset_index(drop=True)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a219b211",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_lookup = summary_df.set_index(\"cluster\")[\"topic\"].to_dict()\n",
    "chunk_df[\"topic\"] = chunk_df[\"cluster\"].map(topic_lookup)\n",
    "chunk_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Embed summaries and re-cluster\n",
    "\n",
    "Now we re-embed the cluster summaries, reduce them with PCA, and run another round of clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_vectors = np.array(embeddings_model.embed_documents(summary_df[\"summary\"].tolist()))\n",
    "summary_pca = PCA(n_components=2, random_state=42)\n",
    "summary_coords = summary_pca.fit_transform(summary_vectors)\n",
    "summary_df[[\"pc1\", \"pc2\"]] = summary_coords\n",
    "\n",
    "summary_clusters = min(3, len(summary_df))\n",
    "summary_kmeans = KMeans(n_clusters=summary_clusters, random_state=42, n_init=\"auto\")\n",
    "summary_df[\"cluster_lvl2\"] = summary_kmeans.fit_predict(summary_vectors)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=summary_df,\n",
    "    x=\"pc1\",\n",
    "    y=\"pc2\",\n",
    "    hue=\"cluster_lvl2\",\n",
    "    palette=\"tab10\",\n",
    "    s=200,\n",
    ")\n",
    "for _, row in summary_df.iterrows():\n",
    "    plt.text(row[\"pc1\"] + 0.01, row[\"pc2\"] + 0.01, f\"C{row['cluster']}\")\n",
    "\n",
    "plt.title(\"Cluster summaries projected & clustered again\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Root summary (summary of summaries)\n",
    "\n",
    "The final RAPTOR node combines the intermediate summaries into one high-level synopsis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You produce an executive summary that captures every cluster.\"),\n",
    "    (\n",
    "        \"user\",\n",
    "        \"Create a single coherent summary of these cluster summaries, highlighting major themes and transitions.\\n\\n{summaries}\",\n",
    "    ),\n",
    "])\n",
    "\n",
    "root_response = llm.invoke(\n",
    "    root_prompt.format_messages(\n",
    "        summaries=\"\\n\\n\".join(\n",
    "            f\"Cluster {row.cluster}: {row.summary}\" for row in summary_df.itertuples()\n",
    "        )\n",
    "    )\n",
    ")\n",
    "root_summary = root_response.content.strip()\n",
    "root_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2947b000",
   "metadata": {},
   "source": [
    "## 10. Persist chunk embeddings in Chroma\n",
    "\n",
    "Wir verwenden [Chroma](https://docs.trychroma.com/getting-started) als persistenten Vektorspeicher. Die Bibliothek ist quelloffen (Apache 2.0) und l?sst sich ohne weitere Credentials lokal betreiben. Optional kann Chroma Cloud genutzt werden ? dann m?ssen `CHROMA_TENANT`, `CHROMA_DATABASE` und `CHROMA_API_KEY` gesetzt werden. F?r dieses Notebook reicht jedoch der lokale Persist-Modus, sodass neu hinzugef?gte Dokumente direkt gespeichert bleiben und nicht erneut eingebettet werden m?ssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd21d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path as _Path\n",
    "import shutil\n",
    "\n",
    "persist_directory = _Path(\"chroma_langchain_db\")\n",
    "if persist_directory.exists():\n",
    "    shutil.rmtree(persist_directory)\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"raptor_chunks_demo\",\n",
    "    embedding_function=embeddings_model,\n",
    "    persist_directory=str(persist_directory),\n",
    ")\n",
    "\n",
    "def _rows_to_documents(df: pd.DataFrame) -> list[Document]:\n",
    "    docs = []\n",
    "    for row in df.itertuples():\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=row.text,\n",
    "                metadata={\n",
    "                    \"chunk_id\": int(row.chunk_id),\n",
    "                    \"cluster\": int(row.cluster),\n",
    "                    \"topic\": row.topic or topic_lookup.get(int(row.cluster)),\n",
    "                    \"source\": document_path.name,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "    return docs\n",
    "\n",
    "base_docs = _rows_to_documents(chunk_df)\n",
    "base_ids = [f\"chunk-{doc.metadata['chunk_id']}\" for doc in base_docs]\n",
    "vector_store.add_documents(documents=base_docs, ids=base_ids)\n",
    "print(f\"Persisted {len(base_docs)} chunks to Chroma at {persist_directory.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6592cfd",
   "metadata": {},
   "source": [
    "## 11. Inkrementelle Updates (adRAP-Style)\n",
    "\n",
    "Statt den kompletten RAPTOR-Baum zu rekonstruieren, simulieren wir den in arXiv:2410.01736 beschriebenen **adRAP**-Ablauf: Neue Dokumente werden in denselben UMAP/GMM-Raum projiziert, dem wahrscheinlichsten Cluster zugewiesen, dort eingebettet und nur die betroffenen Cluster-Zusammenfassungen werden neu generiert. Der Chroma-Vektorspeicher dient dabei als langlebige Quelle f?r alle Chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1ca4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _summarize_cluster(cluster_id: int) -> tuple[str, str]:\n",
    "    \"\"\"Recompute topic + summary for a given cluster.\"\"\"\n",
    "    group = chunk_df[chunk_df[\"cluster\"] == cluster_id]\n",
    "    if group.empty:\n",
    "        return topic_lookup.get(cluster_id, f\"Cluster {cluster_id}\"), \"\"\n",
    "    cluster_text = \"\n",
    "\n",
    "\".join(group[\"text\"].tolist())\n",
    "    response = llm.invoke(prompt.format_messages(cluster_text=cluster_text))\n",
    "    topic, summary_text = _parse_cluster_summary(response.content, cluster_id)\n",
    "    summary_df.loc[summary_df[\"cluster\"] == cluster_id, \"topic\"] = topic\n",
    "    summary_df.loc[summary_df[\"cluster\"] == cluster_id, \"summary\"] = summary_text\n",
    "    topic_lookup[cluster_id] = topic\n",
    "    return topic, summary_text\n",
    "\n",
    "\n",
    "def add_texts_incrementally(raw_texts: list[str], source_label: str = \"manual\") -> list[int]:\n",
    "    \"\"\"Add new documents without rebuilding the entire RAPTOR stack.\"\"\"\n",
    "    global chunk_df\n",
    "    if not raw_texts:\n",
    "        return []\n",
    "\n",
    "    new_rows = []\n",
    "    for text in raw_texts:\n",
    "        for chunk_text in text_splitter.split_text(text):\n",
    "            next_id = int(chunk_df[\"chunk_id\"].max()) + 1 if not chunk_df.empty else 0\n",
    "            vector = embeddings_model.embed_documents([chunk_text])[0]\n",
    "            assigned_cluster = int(kmeans.predict([vector])[0])\n",
    "            topic = topic_lookup.get(assigned_cluster, f\"Cluster {assigned_cluster}\")\n",
    "            doc = Document(\n",
    "                page_content=chunk_text,\n",
    "                metadata={\n",
    "                    \"chunk_id\": next_id,\n",
    "                    \"cluster\": assigned_cluster,\n",
    "                    \"topic\": topic,\n",
    "                    \"source\": source_label,\n",
    "                },\n",
    "            )\n",
    "            vector_store.add_documents(documents=[doc], ids=[f\"chunk-{next_id}\"])\n",
    "            new_rows.append(\n",
    "                {\n",
    "                    \"chunk_id\": next_id,\n",
    "                    \"text\": chunk_text,\n",
    "                    \"cluster\": assigned_cluster,\n",
    "                    \"topic\": topic,\n",
    "                }\n",
    "            )\n",
    "    if not new_rows:\n",
    "        return []\n",
    "\n",
    "    chunk_df = pd.concat([chunk_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    affected_clusters = sorted({row[\"cluster\"] for row in new_rows})\n",
    "    for cluster_id in affected_clusters:\n",
    "        _summarize_cluster(cluster_id)\n",
    "    return affected_clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c0480",
   "metadata": {},
   "source": [
    "### Beispiel: Neues Dokument hinzuf?gen\n",
    "\n",
    "Wir f?gen einen zus?tzlichen Programmier-Abschnitt hinzu. Nur der zugeh?rige Cluster wird aktualisiert; alle anderen bleiben unver?ndert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_update = \"\"\"Programming languages evolve quickly, and teams often need to add changelog entries without rebuilding their knowledge graphs. By tracking cluster centroids and summaries incrementally, we can capture these updates while keeping prior context intact.\"\"\"\n",
    "affected = add_texts_incrementally([example_update], source_label=\"incremental-demo\")\n",
    "print(\"Aktualisierte Cluster:\", affected)\n",
    "summary_df[summary_df[\"cluster\"].isin(affected)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7237a2",
   "metadata": {},
   "source": [
    "## 12. Themen?bersicht & Retrieval\n",
    "\n",
    "Damit wir jederzeit _?Zu welchen Themen hast du Informationen??_ beantworten k?nnen, lesen wir die Themen aus den Clusterzusammenfassungen und ? falls gew?nscht ? f?hren eine schnelle ?hnlichkeitssuche ?ber Chroma aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227d11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_available_topics() -> list[str]:\n",
    "    topics = sorted({topic for topic in topic_lookup.values() if topic})\n",
    "    for topic in topics:\n",
    "        print(f\"- {topic}\")\n",
    "    return topics\n",
    "\n",
    "available_topics = list_available_topics()\n",
    "print(f\"\n",
    "Anzahl Topics: {len(available_topics)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ff0e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Zu welchen Themen hast du Informationen?\"\n",
    "results = vector_store.similarity_search(query, k=3)\n",
    "for doc in results:\n",
    "    print(f\"Topic: {doc.metadata.get('topic')} | Source: {doc.metadata.get('source')}\")\n",
    "    print(doc.page_content[:200].strip(), \"...\n",
    "\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raptor (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
