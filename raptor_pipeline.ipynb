{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAPTOR-Style Hierarchical Summarization Demo\n",
    "\n",
    "This notebook walks through a lightweight RAPTOR workflow over the long-form text in `themen_text.txt`. We will:\n",
    "\n",
    "1. Split the document into manageable chunks.\n",
    "2. Embed the chunks with OpenAI embeddings (no vector store needed).\n",
    "3. Run PCA to project embeddings to 2D and visualize them.\n",
    "4. Cluster the chunks, visualize the clusters, and summarize each cluster with an LLM.\n",
    "5. Embed the summaries, cluster them again, and create a \"summary of summaries\" (the RAPTOR root).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4ad2c9",
   "metadata": {},
   "source": [
    "> **adRAP vs. classical RAPTOR**  \n",
    "> Recent work (arXiv:2410.01736) shows how **adRAP** maintains RAPTOR's recursive tree structure for dynamic datasets. Instead of rebuilding the entire tree whenever new documents arrive, adaptive UMAP/GMM inserts chunks only into the affected clusters and regenerates just those summaries. This notebook follows that idea: we keep chunks in Chroma, update only the relevant clusters, and can always answer questions such as _\"Which topics do you have information about?\"_ without triggering a full rebuild.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install dependencies (one-time)\n",
    "\n",
    "Install the required packages with the following command if they are not already available in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    langchain-openai \\\n",
    "    langchain-text-splitters \\\n",
    "    langchain-core \\\n",
    "    openai \\\n",
    "    matplotlib \\\n",
    "    seaborn \\\n",
    "    scikit-learn \\\n",
    "    pandas \\\n",
    "    numpy \\\n",
    "    python-dotenv \\\n",
    "    langchain-chroma>=0.1.2 \\\n",
    "    chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment & imports\n",
    "\n",
    "Store your API keys securely via environment variables. The prompts below only run if a key is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the source document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_path = Path(\"themen_text.txt\")\n",
    "document = document_path.read_text(encoding=\"utf-8\")\n",
    "print(f\"Loaded {document_path} with {len(document.split())} words\")\n",
    "document[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split into chunks\n",
    "\n",
    "We use the requested `RecursiveCharacterTextSplitter` with `chunk_size=100` and `chunk_overlap=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "chunks = text_splitter.split_text(document)\n",
    "chunk_df = pd.DataFrame({\"chunk_id\": range(len(chunks)), \"text\": chunks})\n",
    "chunk_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embed chunks (no vector store)\n",
    "\n",
    "We use `text-embedding-3-large` via `OpenAIEmbeddings`. This cell may take a while depending on the number of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "chunk_vectors = np.array(embeddings_model.embed_documents(chunk_df[\"text\"].tolist()))\n",
    "chunk_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dimensionality reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=42)\n",
    "chunk_coords = pca.fit_transform(chunk_vectors)\n",
    "chunk_df[[\"pc1\", \"pc2\"]] = chunk_coords\n",
    "chunk_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(chunk_df[\"pc1\"], chunk_df[\"pc2\"], s=30, alpha=0.7)\n",
    "plt.title(\"Chunk embeddings projected with PCA\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhouette-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def best_k_by_silhouette(X, k_min=2, k_max=15, random_state=42):\n",
    "    scores = {}\n",
    "    for k in range(k_min, k_max + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=\"auto\")\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        score = silhouette_score(X, labels)\n",
    "        scores[k] = score\n",
    "        print(f\"k={k}: silhouette={score:.4f}\")\n",
    "    # bestes k zurÃ¼ckgeben\n",
    "    best_k = max(scores, key=scores.get)\n",
    "    return best_k, scores\n",
    "\n",
    "best_k, scores = best_k_by_silhouette(chunk_vectors, k_min=2, k_max=10)\n",
    "print(\"Bestes k nach Silhouette:\", best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clustering the chunks\n",
    "\n",
    "We keep it simple with K-Means. Adjust `n_clusters` to taste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 3\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=\"auto\")\n",
    "chunk_df[\"cluster\"] = kmeans.fit_predict(chunk_vectors)\n",
    "chunk_df.groupby(\"cluster\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(\n",
    "    data=chunk_df,\n",
    "    x=\"pc1\",\n",
    "    y=\"pc2\",\n",
    "    hue=\"cluster\",\n",
    "    palette=\"tab10\",\n",
    "    s=50,\n",
    ")\n",
    "plt.title(\"Chunk clusters after PCA\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summarize each cluster with an LLM\n",
    "\n",
    "The RAPTOR approach compresses each cluster into a higher-level node. We feed the concatenated chunk texts to a chat model and request a concise summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def _parse_cluster_summary(raw_text: str, cluster_id: int) -> tuple[str, str]:\n",
    "    \"\"\"Return (topic, summary_text).\"\"\"\n",
    "    text = raw_text.strip()\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "        topic = data.get(\"topic\") or data.get(\"title\") or f\"Cluster {cluster_id}\"\n",
    "        summary_text = data.get(\"summary\") or data.get(\"content\") or text\n",
    "        return topic.strip(), summary_text.strip()\n",
    "    except json.JSONDecodeError:\n",
    "        # fallback: first sentence as topic\n",
    "        parts = text.split(\"\\n\", 1)\n",
    "        topic = parts[0][:80] if parts else f\"Cluster {cluster_id}\"\n",
    "        summary_text = text if len(parts) == 1 else parts[1]\n",
    "        return topic.strip() or f\"Cluster {cluster_id}\", summary_text.strip()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Summarize clusters and respond as compact JSON with keys 'topic' and 'summary'.\"),\n",
    "    (\n",
    "        \"user\",\n",
    "        \"You will receive the merged text for one cluster. Respond as JSON with keys 'topic' (<=8 words) and 'summary' (<=2 sentences, max 60 words) that briefly capture the essential idea.\\n\\n{cluster_text}\",\n",
    "    ),\n",
    "])\n",
    "\n",
    "cluster_summaries = []\n",
    "\n",
    "for cluster_id, group in chunk_df.groupby(\"cluster\"):\n",
    "    cluster_text = \"\\n\\n\".join(group[\"text\"].tolist())\n",
    "    response = llm.invoke(prompt.format_messages(cluster_text=cluster_text))\n",
    "    topic, summary_text = _parse_cluster_summary(response.content, int(cluster_id))\n",
    "    cluster_summaries.append(\n",
    "        {\n",
    "            \"cluster\": int(cluster_id),\n",
    "            \"topic\": topic,\n",
    "            \"summary\": summary_text,\n",
    "            \"raw\": response.content.strip(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "summary_df = pd.DataFrame(cluster_summaries).sort_values(\"cluster\").reset_index(drop=True)\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a219b211",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_lookup = summary_df.set_index(\"cluster\")[\"topic\"].to_dict()\n",
    "chunk_df[\"topic\"] = chunk_df[\"cluster\"].map(topic_lookup)\n",
    "chunk_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Embed summaries and re-cluster\n",
    "\n",
    "Now we re-embed the cluster summaries, reduce them with PCA, and run another round of clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_vectors = np.array(embeddings_model.embed_documents(summary_df[\"summary\"].tolist()))\n",
    "summary_pca = PCA(n_components=2, random_state=42)\n",
    "summary_coords = summary_pca.fit_transform(summary_vectors)\n",
    "summary_df[[\"pc1\", \"pc2\"]] = summary_coords\n",
    "\n",
    "summary_clusters = min(3, len(summary_df))\n",
    "summary_kmeans = KMeans(n_clusters=summary_clusters, random_state=42, n_init=\"auto\")\n",
    "summary_df[\"cluster_lvl2\"] = summary_kmeans.fit_predict(summary_vectors)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=summary_df,\n",
    "    x=\"pc1\",\n",
    "    y=\"pc2\",\n",
    "    hue=\"cluster_lvl2\",\n",
    "    palette=\"tab10\",\n",
    "    s=200,\n",
    ")\n",
    "for _, row in summary_df.iterrows():\n",
    "    plt.text(row[\"pc1\"] + 0.01, row[\"pc2\"] + 0.01, f\"C{row['cluster']}\")\n",
    "\n",
    "plt.title(\"Cluster summaries projected & clustered again\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Root summary (summary of summaries)\n",
    "\n",
    "The final RAPTOR node combines the intermediate summaries into one high-level synopsis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You produce an executive summary that captures every cluster.\"),\n",
    "    (\n",
    "        \"user\",\n",
    "        \"List each cluster in bullet form (`-`). For every cluster, include the topic name and one concise sentence (<=40 words). Provide at most 5 bullets.\\n\\n{summaries}\",\n",
    "    ),\n",
    "])\n",
    "\n",
    "root_response = llm.invoke(\n",
    "    root_prompt.format_messages(\n",
    "        summaries=\"\\n\\n\".join(\n",
    "            f\"Cluster {row.cluster}: {row.summary}\" for row in summary_df.itertuples()\n",
    "        )\n",
    "    )\n",
    ")\n",
    "root_summary = root_response.content.strip()\n",
    "root_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d83be",
   "metadata": {},
   "source": [
    "### Persist Root Summary for External Agents\n",
    "\n",
    "To keep the A2A agent card aligned with the current root summary, we store `root_summary` in a text file. The A2A server loads this file during startup and copies the text into `AgentCard.description`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48e38e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_output_path = Path(\"artifacts/root_summary.txt\")\n",
    "summary_output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "summary_output_path.write_text(root_summary.strip(), encoding=\"utf-8\")\n",
    "print(f\"Root summary persisted to {summary_output_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2947b000",
   "metadata": {},
   "source": [
    "## 10. Persist chunk embeddings in Chroma\n",
    "\n",
    "We use [Chroma](https://docs.trychroma.com/getting-started) as a persistent vector store. The library is open source (Apache 2.0) and runs locally without credentials. If you want to use Chroma Cloud, set `CHROMA_TENANT`, `CHROMA_DATABASE`, and `CHROMA_API_KEY`. For this notebook the local persist mode is enough, so newly added documents are stored immediately and do not need to be re-embedded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd21d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path as _Path\n",
    "import shutil\n",
    "\n",
    "persist_directory = _Path(\"chroma_langchain_db\")\n",
    "if persist_directory.exists():\n",
    "    shutil.rmtree(persist_directory)\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"raptor_chunks_demo\",\n",
    "    embedding_function=embeddings_model,\n",
    "    persist_directory=str(persist_directory),\n",
    ")\n",
    "\n",
    "def _rows_to_documents(df: pd.DataFrame) -> list[Document]:\n",
    "    docs = []\n",
    "    for row in df.itertuples():\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=row.text,\n",
    "                metadata={\n",
    "                    \"chunk_id\": int(row.chunk_id),\n",
    "                    \"cluster\": int(row.cluster),\n",
    "                    \"topic\": row.topic or topic_lookup.get(int(row.cluster)),\n",
    "                    \"source\": document_path.name,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "    return docs\n",
    "\n",
    "base_docs = _rows_to_documents(chunk_df)\n",
    "base_ids = [f\"chunk-{doc.metadata['chunk_id']}\" for doc in base_docs]\n",
    "vector_store.add_documents(documents=base_docs, ids=base_ids)\n",
    "print(f\"Persisted {len(base_docs)} chunks to Chroma at {persist_directory.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6592cfd",
   "metadata": {},
   "source": [
    "## 11. Incremental Updates (adRAP-Style)\n",
    "\n",
    "Instead of reconstructing the entire RAPTOR tree, we emulate the **adRAP** procedure from arXiv:2410.01736: new documents are projected into the existing UMAP/GMM space, assigned to the most likely cluster, embedded there, and only the affected cluster summaries are regenerated. The Chroma vector store stays the durable source of truth for all chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1ca4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _summarize_cluster(cluster_id: int) -> tuple[str, str]:\n",
    "    \"\"\"Recompute topic + summary for a given cluster.\"\"\"\n",
    "    group = chunk_df[chunk_df[\"cluster\"] == cluster_id]\n",
    "    if group.empty:\n",
    "        return topic_lookup.get(cluster_id, f\"Cluster {cluster_id}\"), \"\"\n",
    "    cluster_text = \"\".join(group[\"text\"].tolist())\n",
    "    response = llm.invoke(prompt.format_messages(cluster_text=cluster_text))\n",
    "    topic, summary_text = _parse_cluster_summary(response.content, cluster_id)\n",
    "    summary_df.loc[summary_df[\"cluster\"] == cluster_id, \"topic\"] = topic\n",
    "    summary_df.loc[summary_df[\"cluster\"] == cluster_id, \"summary\"] = summary_text\n",
    "    topic_lookup[cluster_id] = topic\n",
    "    return topic, summary_text\n",
    "\n",
    "\n",
    "def add_texts_incrementally(raw_texts: list[str], source_label: str = \"manual\") -> list[int]:\n",
    "    \"\"\"Add new documents without rebuilding the entire RAPTOR stack.\"\"\"\n",
    "    global chunk_df\n",
    "    if not raw_texts:\n",
    "        return []\n",
    "\n",
    "    new_rows = []\n",
    "    for text in raw_texts:\n",
    "        for chunk_text in text_splitter.split_text(text):\n",
    "            next_id = int(chunk_df[\"chunk_id\"].max()) + 1 if not chunk_df.empty else 0\n",
    "            vector = embeddings_model.embed_documents([chunk_text])[0]\n",
    "            assigned_cluster = int(kmeans.predict([vector])[0])\n",
    "            topic = topic_lookup.get(assigned_cluster, f\"Cluster {assigned_cluster}\")\n",
    "            doc = Document(\n",
    "                page_content=chunk_text,\n",
    "                metadata={\n",
    "                    \"chunk_id\": next_id,\n",
    "                    \"cluster\": assigned_cluster,\n",
    "                    \"topic\": topic,\n",
    "                    \"source\": source_label,\n",
    "                },\n",
    "            )\n",
    "            vector_store.add_documents(documents=[doc], ids=[f\"chunk-{next_id}\"])\n",
    "            new_rows.append(\n",
    "                {\n",
    "                    \"chunk_id\": next_id,\n",
    "                    \"text\": chunk_text,\n",
    "                    \"cluster\": assigned_cluster,\n",
    "                    \"topic\": topic,\n",
    "                }\n",
    "            )\n",
    "    if not new_rows:\n",
    "        return []\n",
    "\n",
    "    chunk_df = pd.concat([chunk_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    affected_clusters = sorted({row[\"cluster\"] for row in new_rows})\n",
    "    for cluster_id in affected_clusters:\n",
    "        _summarize_cluster(cluster_id)\n",
    "    return affected_clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c0480",
   "metadata": {},
   "source": [
    "### Example: Add a New Document\n",
    "\n",
    "We insert an additional programming paragraph. Only the corresponding cluster is updated; all other clusters remain untouched.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_update = \"\"\"Programming languages evolve quickly, and teams often need to add changelog entries without rebuilding their knowledge graphs. By tracking cluster centroids and summaries incrementally, we can capture these updates while keeping prior context intact.\"\"\"\n",
    "affected = add_texts_incrementally([example_update], source_label=\"incremental-demo\")\n",
    "print(\"Aktualisierte Cluster:\", affected)\n",
    "summary_df[summary_df[\"cluster\"].isin(affected)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7237a2",
   "metadata": {},
   "source": [
    "## 12. Topic Overview & Retrieval\n",
    "\n",
    "To answer _\"Which topics do you have information about?\"_ at any time, we read the topics from the cluster summaries and optionally run a quick similarity search against Chroma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227d11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_available_topics() -> list[str]:\n",
    "    topics = sorted({topic for topic in topic_lookup.values() if topic})\n",
    "    for topic in topics:\n",
    "        print(f\"- {topic}\")\n",
    "    return topics\n",
    "\n",
    "available_topics = list_available_topics()\n",
    "print(f\"Anzahl Topics: {len(available_topics)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ff0e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Zu welchen Themen hast du Informationen?\"\n",
    "results = vector_store.similarity_search(query, k=3)\n",
    "for doc in results:\n",
    "    print(f\"Topic: {doc.metadata.get('topic')} | Source: {doc.metadata.get('source')}\")\n",
    "    print(doc.page_content[:200].strip(), \"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raptor (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
